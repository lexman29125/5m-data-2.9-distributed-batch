{"cells": [{"cell_type": "markdown", "metadata": {"id": "Yf22YCkqc0Fh"}, "source": "# Hands-on with Spark - Google Dataproc"}, {"cell_type": "markdown", "metadata": {"id": "B96jjJtWf6Z1"}, "source": "![spark](https://cdn-images-1.medium.com/max/300/1*c8CtvqKJDVUnMoPGujF5fA.png)"}, {"cell_type": "markdown", "metadata": {"id": "GRrcTfEGc8Z2"}, "source": "PySpark is the Python API of Spark; which means it can do almost all the things python can- Machine learning (ML), exploratory data analysis (EDA), ETLs for data platform. And all of them in a distributed manner.\n\n![pyspark](https://editor.analyticsvidhya.com/uploads/20981sp3.JPG)\n\nIn simple terms, each time you submit a PySpark job, the code gets internally converted into a MapReduce program and gets executed in the Java Virtual Machine. Spark also uses Lazy Evaluation, it delays its evaluation as much as it can. Each time you submit a job, spark creates an action plan for how to execute the code, and then does nothing. Finally, when you ask for the result (i.e, calls an action), it executes the plan, which is basically all the transofrmations you have mentioned in your code.\n"}, {"cell_type": "markdown", "metadata": {"id": "h5598w-HtOci"}, "source": "For our lesson, we will be running Spark on **local** mode.\n\nIn this mode, Spark runs on a single machine, utilizing available cores (similar to Polars).\nIt's useful for development, learning, testing, and debugging since everything runs on the local machine without needing a cluster.\n\nIn actual production environments, Spark is usually run on a cluster of machines (YARN, Mesos or Kubernetes mode)."}, {"cell_type": "markdown", "metadata": {"id": "j_yU4yEjvLH7"}, "source": "## Initializing Spark"}, {"cell_type": "markdown", "metadata": {}, "source": "# For mult-node spark, call Spark session with Yarn "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"DistributedSparkApp\") \\\n    .master(\"yarn\") \\\n    .config(\"spark.executor.instances\", \"2\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.executor.memory\", \"2g\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .getOrCreate()"}, {"cell_type": "markdown", "metadata": {"id": "OFuPn8O_wWRx"}, "source": "Invoking `spark` will print the SparkContext"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 221}, "id": "TwirdxMKv-ie", "outputId": "bbe45d15-7468-47ae-b34d-3308f2618cad"}, "outputs": [], "source": "spark"}, {"cell_type": "markdown", "metadata": {"id": "vlvH2gCppT3f"}, "source": "## Spark SQL and DataFrames\n\nSpark SQL and DataFrames are higher-level modules of Apache Spark, they work together to make data processing with Spark more intuitive and optimized, especially for those who come from an SQL or data analytics background.\n\n- **SQL Interface to Spark**: Spark SQL lets users execute SQL queries alongside Spark programs.\n\n- **DataFrames are abstraction over RDDs**: A DataFrame is a distributed collection of data organized into named columns. Conceptually, it's equivalent to a table in a relational database or a data frame in R or Python, but with optimizations for distributed processing and scalability. While RDD (Resilient Distributed Dataset) is a fundamental data structure in Spark, DataFrames provide a higher-level abstraction that is often easier to use and more optimized for many tasks."}, {"cell_type": "markdown", "metadata": {"id": "mL9FT69RtlhD"}, "source": "### DataFrame Creation"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Csj4V7SebG3-"}, "outputs": [], "source": "from datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "bUTCVOCMbZHs", "outputId": "352cf986-5a17-4067-8524-12c270137a56"}, "outputs": [], "source": "df = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf"}, {"cell_type": "markdown", "metadata": {"id": "o3pCzfH8t85j"}, "source": "PySpark infers the DataFrame schema (dtypes) by taking a sample from the data (just like Pandas)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "jM049Yg3bfei", "outputId": "4de00b5b-12cd-452a-d85a-c0170ed9eee1"}, "outputs": [], "source": "df.show(5)"}, {"cell_type": "markdown", "metadata": {"id": "hDh6bDwPuHuf"}, "source": "\nYou can also pass an explicit schema:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "DPIuxbgVuJ7L", "outputId": "54d568e6-81d7-4394-eb64-88ff4818dec4"}, "outputs": [], "source": "df = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf"}, {"cell_type": "markdown", "metadata": {"id": "gELgm_S3NGbs"}, "source": "PySpark DataFrame is lazily evaluated and invoking `df` does not trigger the computation and show anything. You need to explicitly call the `show` method:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "WztTVjujuRsj", "outputId": "9622599a-dca0-440b-a262-1452ed3d2abe"}, "outputs": [], "source": "df.show()\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {"id": "7l6zXe8wujkC"}, "source": "### Getting Data In/Out\n\nThere are many data sources available in PySpark such as CSV, Parquet, ORC, JDBC, text, Avro, etc.\n\nFirst, let's download the files in various format from GCS, data which we explored in Unit 2.3 and 2.8."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "r2TEii0S7_JN"}, "outputs": [], "source": "!mkdir -p data"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "LrUfmub78XZR", "outputId": "55f2ef78-51cc-4503-e94a-646baf16ba3b"}, "outputs": [], "source": "!wget -P data https://storage.googleapis.com/su-artifacts/movies_metadata.csv\n!wget -P data https://storage.googleapis.com/su-artifacts/ratings.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!wget -P data https://storage.googleapis.com/su-artifacts/taxi_trip_data.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!wget -P data https://storage.googleapis.com/su-artifacts/userdata1.orc\n!wget -P data https://storage.googleapis.com/su-artifacts/userdata1.parquet"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "2_zUlgQ4cILE", "outputId": "2e384389-fea4-405d-d97c-76dc0276fa80"}, "outputs": [], "source": "!ls /data"}, {"cell_type": "markdown", "metadata": {}, "source": "## Move downloaded files from local disk to hadoop file system"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!hdfs dfs -put /data /user/"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!hdfs dfs -ls /user/data"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "$"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "5odksbT69b2J", "outputId": "ad87b231-f412-4bf5-fc9f-068d06bed3e9"}, "outputs": [], "source": "df = spark.read.parquet('/user/data/userdata1.parquet')\ndf.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "I37la4QA-TMm", "outputId": "29ec9b33-7eca-4037-fade-d9c20d25a589"}, "outputs": [], "source": "df = spark.read.orc('/user/data/userdata1.orc')\ndf.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "miuJSiRD_eA2"}, "outputs": [], "source": "movies = spark.read.csv('/user/data/movies_metadata.csv', header=True)\nratings = spark.read.csv('/user/data/ratings.csv', header=True)\ntaxi = spark.read.csv('/user/data/taxi_trip_data.csv', header=True)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Hbw45ZcZ_4Bn", "outputId": "288ed543-c039-4015-f1e7-fc820013253c"}, "outputs": [], "source": "movies.printSchema()"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "sz0jjCFO_wcU", "outputId": "42a14ac3-fde5-438c-bf5b-bec53ad71284"}, "outputs": [], "source": "ratings.printSchema()"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "fC-BED54G3lF", "outputId": "72bed946-e9a9-428b-eb82-eda0f6673a5e"}, "outputs": [], "source": "taxi.printSchema()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "IWec2yxrG7yp"}, "outputs": [], "source": "movies = spark.read.csv('/user/data/movies_metadata.csv', header=True, inferSchema=True, quote=\"\\\"\", escape=\"\\\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "XgVmnJFdG_Et", "outputId": "0629645d-2e05-45ad-8c97-e5e7f87d0c7b"}, "outputs": [], "source": "movies.printSchema()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "rxXMe5tdAhhI"}, "outputs": [], "source": "ratings = spark.read.csv('/user/data/ratings.csv', header=True, inferSchema=True)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "gmfe5kwUArq-", "outputId": "bc49aa91-0851-4b7a-950f-f46d7ad5cf30"}, "outputs": [], "source": "ratings.printSchema()"}, {"cell_type": "markdown", "metadata": {"id": "9a3R2ZE1HUFZ"}, "source": "Passing a schema will speed up the read."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "1OuIC8ObHy0s"}, "outputs": [], "source": "dtypes = ['vendor_id integer',\n 'pickup_datetime timestamp',\n 'dropoff_datetime timestamp',\n 'passenger_count integer',\n 'trip_distance double',\n 'rate_code integer',\n 'store_and_fwd_flag string',\n 'payment_type integer',\n 'fare_amount double',\n 'extra double',\n 'mta_tax double',\n 'tip_amount double',\n 'tolls_amount double',\n 'imp_surcharge double',\n 'total_amount double',\n 'pickup_location_id integer',\n 'dropoff_location_id integer']"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "5JbmrKbiHKdc"}, "outputs": [], "source": "taxi = spark.read.csv('/user/data/taxi_trip_data.csv', header=True,\n                      schema=', '.join(dtypes))"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "DEuzBCPRAFMY", "outputId": "fa277ca4-cdcf-433a-99b9-b33519f55861"}, "outputs": [], "source": "taxi.printSchema()"}, {"cell_type": "markdown", "metadata": {"id": "DhNpGCZHI5Yy"}, "source": "There are a couple of ways to view your dataframe in PySpark:\n\n1.   `df.take(5)` will return a list of five Row objects.\n2.   `df.collect()` will get all of the data from the entire DataFrame. Be really careful when using it, because if you have a large data set, you can easily crash the driver node.\n3.   `df.show()` is the most commonly used method to view a dataframe. There are a few parameters we can pass to this method, like the number of rows and truncaiton. For example, `df.show(5, False)` or ` df.show(5, truncate=False)` will show the entire data wihtout any truncation.\n4.   `df.limit(5)` will **return a new DataFrame** by taking the first n rows. As spark is distributed in nature, there is no guarantee that `df.limit()` will give you the same results each time."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "6vYO64dNIoYD", "outputId": "702cb163-8d8d-4000-d9e8-b0d3598deeec"}, "outputs": [], "source": "movies.show(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Ku5Ohaa6JAa7", "outputId": "f5186b17-01df-47b7-f3ee-a931c9bda15b"}, "outputs": [], "source": "movies.show(5, truncate=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "sj1hzCXZKCHV"}, "outputs": [], "source": "# can also be shown vertically\nmovies.show(1, vertical=True, truncate=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "WiqfEY-1Ir0j"}, "outputs": [], "source": "ratings.show(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "SRXESwE4Itf6"}, "outputs": [], "source": "taxi.show(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "aG5UlLVLIypN"}, "outputs": [], "source": "taxi.limit(5).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "uYinFUS3KXYo"}, "outputs": [], "source": "movies.describe().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "xT1dm9WRKmHB"}, "outputs": [], "source": "ratings.describe().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "OUmLPl3LKnDq"}, "outputs": [], "source": "taxi.describe().show()"}, {"cell_type": "markdown", "metadata": {"id": "srqwAMFMJkU0"}, "source": "### DataFrame Operations on Columns\n\nWe will go over the following in this section:\n\n1.   Selecting a Column\n2.   Selecting Multiple Columns\n3.   Adding New Columns\n4.   Renaming Columns\n5.   Removing Columns\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "TF0Z0RqeJjp8"}, "outputs": [], "source": "movies.id"}, {"cell_type": "markdown", "metadata": {"id": "o59d97i2NnKb"}, "source": "Just like DataFrame, selecting a column is lazily evaluated and does not trigger the computation but returns a `Column` instance.\n\nIt can be used to select columns and returns another DataFrame:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Ydr4uqznNl4d"}, "outputs": [], "source": "movies.select(movies.id).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "G5yp1kdgOPRe"}, "outputs": [], "source": "movies.select(movies.title, movies.overview).show(truncate=False)"}, {"cell_type": "markdown", "metadata": {"id": "gqApk2x-OpEs"}, "source": "Creating a new column:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "6mf9kBX5PQaE"}, "outputs": [], "source": "from pyspark.sql.functions import lit"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pKxyOHWFPSHW"}, "outputs": [], "source": "# lit means literal. It populates the row with the literal value given\nratings.withColumn('review', lit('Great movie!')).show(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "itZlHm53Plh5"}, "outputs": [], "source": "ratings.withColumn('review', lit('Great movie!')) \\\n       .withColumn('mood', lit(5)) \\\n       .show(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "hUdFHvWkOlfw"}, "outputs": [], "source": "movies.withColumnRenamed('overview', 'summary').show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "dqLqpHphQ-l2"}, "outputs": [], "source": "ratings.drop('timestamp').show()"}, {"cell_type": "markdown", "metadata": {"id": "ECtIeRoxSdAB"}, "source": "### Common Transformation Functions"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "a_OmdwOOSm60"}, "outputs": [], "source": "# Functions available in PySpark\nfrom pyspark.sql import functions\n# We can use the dir function to view the available functions\nprint(dir(functions))"}, {"cell_type": "markdown", "metadata": {"id": "d4gsChVFXOHn"}, "source": "String functions:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "SY96OEMERHt9"}, "outputs": [], "source": "from pyspark.sql.functions import concat, lower, upper, substring"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "EAeEaV9WS81k"}, "outputs": [], "source": "movies.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "oRP7Qr0WWZZ3"}, "outputs": [], "source": "movies.select(movies.title, movies.tagline, upper(movies.title), lower(movies.tagline),\n              substring(movies.overview, 1, 10),\n              concat(movies.title, lit(' Part 1')).alias('new_title')\n              ).show()"}, {"cell_type": "markdown", "metadata": {"id": "mzltd7IMXQ0J"}, "source": "Numeric functions:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "oxGJmZqYXQW2"}, "outputs": [], "source": "from pyspark.sql.functions import mean, max, round"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "IRDNP5cbXnls"}, "outputs": [], "source": "ratings.select(mean(ratings.rating), max(ratings.rating)).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "4YjWCWIIX01h"}, "outputs": [], "source": "ratings.withColumn('new_rating', round(ratings.rating)).show()"}, {"cell_type": "markdown", "metadata": {"id": "xKKNz1orYNDt"}, "source": "Date/time functions:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "KMVT6lVUYMc3"}, "outputs": [], "source": "from pyspark.sql.functions import to_date, to_timestamp, datediff"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "EVQBTD0kYXQk"}, "outputs": [], "source": "ratings.withColumn('new_timestamp', to_timestamp(ratings.timestamp)).show()"}, {"cell_type": "markdown", "metadata": {"id": "LrkCuPYhYxpb"}, "source": "Replace the unix timestamp with new timestamp:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "b8uWzeuPYrjD"}, "outputs": [], "source": "ratings = ratings.withColumn('timestamp', to_timestamp(ratings.timestamp))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "hGr_8jtLY5Ut"}, "outputs": [], "source": "ratings.show()"}, {"cell_type": "markdown", "metadata": {"id": "yEQdF4vPZYCz"}, "source": "Computing the difference in minutes is more tedious, we'll need to convert the timestamps to unix timestamps (seconds since epoch), compute the difference, and divide by 60."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "rH-rd1VrZJ03"}, "outputs": [], "source": "taxi = taxi.withColumn('trip_duration', (taxi.dropoff_datetime.cast(\"long\") - taxi.pickup_datetime.cast(\"long\"))/60)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "tLciHlrXZ-Q5"}, "outputs": [], "source": "taxi.show(10)"}, {"cell_type": "markdown", "metadata": {"id": "y-M54I7dadI4"}, "source": "### User-Defined Functions (UDF)\n\nPySpark User-Defined Functions (UDFs) help you convert your Python code into a scalable version of itself. It is handy, but beware, as the performance is slower compared to PySpark functions."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "jGlXlPczaxRA"}, "outputs": [], "source": "import pandas as pd\nfrom pyspark.sql.functions import pandas_udf"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "5Jqi_PDWa5Ag"}, "outputs": [], "source": "@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return (series + 1.0).astype(float)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "kLgaPGtwa-3i"}, "outputs": [], "source": "taxi.select(taxi.passenger_count, pandas_plus_one(taxi.passenger_count)).show()"}, {"cell_type": "markdown", "metadata": {"id": "4XninFQBcE4g"}, "source": "### DataFrame Operations on Rows\n\nWe will show the follwoing in this section:\n\n1.   Filtering Rows\n2. \t Get Distinct Rows\n3.   Sorting Rows"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "sQIjymURcEas"}, "outputs": [], "source": "movies.filter(movies.status == 'In Production').show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "JF_42Wqlcu7G"}, "outputs": [], "source": "ratings.filter(ratings.rating > 4.5).show()"}, {"cell_type": "markdown", "metadata": {"id": "zOQvZhSzc6Qi"}, "source": "Use `&` and `|` for combining conditions as `and` and `or`:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "uQCAnd65dBH1"}, "outputs": [], "source": "ratings.filter((ratings.rating < 1.5) | (ratings.rating > 4.5)).show()"}, {"cell_type": "markdown", "metadata": {"id": "YnY8muRUgzAM"}, "source": "> 1. Filter `movies` for `status` to be `In Production` and `vote_average` greater than 6.\n> 2. Filter `taxi` for `trip_duration` greater than 1 hour, `trip_distance` greater than 10 miles and `passenger_count` equal to and less than 2."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Bvd8hGixFnvB"}, "outputs": [], "source": "# 1. Filter movies for status 'In Production' and vote_average > 6\n# This will show movies that are still in production and have a vote_average greater than 6.\n# If this returns no records, it means there are no such movies in your dataset.\nmovies.filter(\n    (movies.status == 'In Production') & (movies.vote_average > 6)\n).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "eTwtKxInFqd_"}, "outputs": [], "source": "# 2. Filter taxi for trip_duration > 1 hour, trip_distance > 10 miles, and passenger_count <= 2\n# First, create the trip_duration column (in minutes) using pickup and dropoff timestamps.\nfrom pyspark.sql.functions import col\n\ntaxi = taxi.withColumn(\n    'trip_duration',\n    (col('dropoff_datetime').cast('long') - col('pickup_datetime').cast('long')) / 60\n)\n\n# Now filter for trips longer than 1 hour, distance > 10 miles, and 2 or fewer passengers.\ntaxi.filter(\n    (col('trip_duration') > 60) &\n    (col('trip_distance') > 10) &\n    (col('passenger_count') <= 2)\n).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "4OudUbxfhwA1"}, "outputs": [], "source": "movies.select('status').distinct().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "IqdCao2Ah5Vm"}, "outputs": [], "source": "taxi.orderBy('trip_duration').show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "NlDn2cw8iCkD"}, "outputs": [], "source": "taxi.orderBy('trip_distance', ascending=False).show()"}, {"cell_type": "markdown", "metadata": {"id": "5nJKlUQDiWvE"}, "source": "### Grouping Data"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "MQBnN-GEi4eq"}, "outputs": [], "source": "movies.groupBy('status').count().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "qsxB74X6ibnE"}, "outputs": [], "source": "ratings.groupBy('movieId').avg('rating').show()"}, {"cell_type": "markdown", "metadata": {"id": "qVsyDwp_jdIT"}, "source": "### Joining DataFrames"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "_c9C3W12keQp"}, "outputs": [], "source": "ratings_avg_by_movie = ratings.groupBy('movieId').avg('rating')"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "-zmJq1wIkzF3"}, "outputs": [], "source": "ratings_avg_by_movie.columns"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "8vPlsNt-jc1n"}, "outputs": [], "source": "movies.join(ratings_avg_by_movie, movies.id == ratings.movieId, 'inner').select('title', 'vote_average', 'avg(rating)').show()"}, {"cell_type": "markdown", "metadata": {"id": "gZT7XuJzjozq"}, "source": "### Spark SQL\n\nDataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "9sPiMKZSjtU2"}, "outputs": [], "source": "taxi.createOrReplaceTempView(\"taxi\")\nspark.sql(\"SELECT count(*) from taxi\").show()"}, {"cell_type": "markdown", "metadata": {"id": "gmCWYSSolXRX"}, "source": "In addition, UDFs can be registered and invoked in SQL out of the box:"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "IniLwEVglQlL"}, "outputs": [], "source": "@pandas_udf(\"double\")\ndef div(s1: pd.Series, s2: pd.Series) -> pd.Series:\n    return s1 / s2"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "hJK58EWFlr1j"}, "outputs": [], "source": "spark.udf.register(\"div\", div)\nspark.sql(\"SELECT trip_distance, trip_duration, div(trip_distance, trip_duration) AS speed FROM taxi\").show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "dt9E0TVANyz4"}, "outputs": [], "source": "taxi.selectExpr(\"div(trip_distance, trip_duration)\").show()"}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}